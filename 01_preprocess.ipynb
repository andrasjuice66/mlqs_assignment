{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-02T19:57:10.993175Z",
     "iopub.status.busy": "2024-05-02T19:57:10.992192Z",
     "iopub.status.idle": "2024-05-02T19:57:11.714611Z",
     "shell.execute_reply": "2024-05-02T19:57:11.714212Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from src.OutlierDetection import DistanceBasedOutlierDetection\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from matplotlib.legend_handler import HandlerPathCollection\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from util.plot_util import plot_lof_2d_grad, plot_lof_2d_circle, plot_lof_3d_grad ,plot_lof_3d_circle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_start_end(df, secs=5):\n",
    "    return df[(df[\"Time (s)\"] > secs) & (df[\"Time (s)\"] < max(df[\"Time (s)\"]) - secs)]\n",
    "\n",
    "\n",
    "def calculate_lof(df, cols, label):\n",
    "    if not all(col in df.columns for col in cols):\n",
    "        missing_cols = [col for col in cols if col not in df.columns]\n",
    "        #print(f\"Missing columns for LOF calculation: {missing_cols}\")\n",
    "        return df\n",
    "    \n",
    "    df[cols] = df[cols].apply(lambda x: x.fillna(x.median()))\n",
    "    #df[cols] = df[cols].interpolate()  \n",
    "    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.2)\n",
    "    y_pred = lof.fit_predict(df[cols])\n",
    "    df[f'lof_{label}'] = y_pred\n",
    "    df[f'lof_factor_{label}'] = -lof.negative_outlier_factor_  # Negate to align with typical plotting conventions\n",
    "    return df\n",
    "\n",
    "def remove_outliers_and_impute(df, cols, label):\n",
    "    df.loc[df[f'lof_{label}'] == -1, cols] = pd.NA\n",
    "    # Forward fill NaNs\n",
    "    df[cols] = df[cols].ffill()\n",
    "    df.drop(columns = [f'lof_{label}', f'lof_factor_{label}'])\n",
    "    return df\n",
    "\n",
    "def correct_column_names(df, corrections):\n",
    "    return df.rename(columns=corrections)\n",
    "\n",
    "def add_missing_datetime_column(folder_path):\n",
    "    # Extract start time from the folder name\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    \n",
    "    try:\n",
    "        start_time_str = ' '.join(folder_name.split()[-2:])\n",
    "        start_time = datetime.strptime(start_time_str, \"%Y-%m-%d %H-%M-%S\")\n",
    "    except ValueError as e:\n",
    "        #print(f\"Skipping folder {folder_path} due to ValueError: {e}\")\n",
    "        return\n",
    "\n",
    "    # Iterate over each CSV file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Skip non-CSV files\n",
    "        if not filename.endswith('.csv'):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Drop rows where 'Time (s)' is missing\n",
    "            df = drop_rows_with_no_time(df)\n",
    "            \n",
    "            # Ensure the Time (s) column exists and the DataFrame is not empty\n",
    "            if 'Time (s)' in df.columns and not df.empty:\n",
    "                df['Datetime'] = df['Time (s)'].apply(lambda x: start_time + timedelta(seconds=x))\n",
    "                df.to_csv(file_path, index=False)\n",
    "                #print(f\"Updated {filename} with Datetime column.\")\n",
    "            else:\n",
    "                print(f\"Skipped {filename}: 'Time (s)' column not found or DataFrame is empty.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "def drop_rows_with_no_time(df):\n",
    "    \"\"\"\n",
    "    Drop rows where there is no value in the 'Time (s)' column.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with rows removed where 'Time (s)' is NaN or missing.\n",
    "    \"\"\"\n",
    "    if 'Time (s)' in df.columns:\n",
    "        #print(df.head)\n",
    "        # Drop rows where 'Time (s)' is NaN or an empty string\n",
    "        df_cleaned = df[df['Time (s)'].notna() & (df['Time (s)'] != '')]\n",
    "        #print(f\"Dropped rows: {len(df) - len(df_cleaned)}\")\n",
    "        return df_cleaned\n",
    "    else:\n",
    "        #print(\"The column 'Time (s)' does not exist in the DataFrame.\")\n",
    "        return df\n",
    "\n",
    "\n",
    "column_corrections = {\n",
    "        \"Accelerometer\": {\n",
    "                \"X (m/s^2)\": \"Acceleration x (m/s^2)\",\n",
    "                \"Y (m/s^2)\": \"Acceleration y (m/s^2)\",\n",
    "                \"Z (m/s^2)\": \"Acceleration z (m/s^2)\"\n",
    "        },\n",
    "        \"Gyroscope\": {\n",
    "                \"X (rad/s)\": \"Gyroscope x (rad/s)\",\n",
    "                \"Y (rad/s)\": \"Gyroscope y (rad/s)\",\n",
    "                \"Z (rad/s)\": \"Gyroscope z (rad/s)\"\n",
    "        },\n",
    "        \"Linear Acceleration\": {\n",
    "                \"X (m/s^2)\": \"Linear Acceleration x (m/s^2)\",\n",
    "                \"Y (m/s^2)\": \"Linear Acceleration y (m/s^2)\",\n",
    "                \"Z (m/s^2)\": \"Linear Acceleration z (m/s^2)\"},\n",
    "         \"Location\": {\n",
    "                \"Vertical Accuracy (°)\": \"Vertical Accuracy (m)\"}}\n",
    "\n",
    "\n",
    "acc_cols = [\"Acceleration x (m/s^2)\", \"Acceleration y (m/s^2)\", \"Acceleration z (m/s^2)\"]\n",
    "loc_height_col = [\"Latitude (°)\", \"Longitude (°)\", \"Height (m)\"]\n",
    "gyro_cols = ['Gyroscope x (rad/s)', 'Gyroscope y (rad/s)', 'Gyroscope z (rad/s)']\n",
    "loc_cols = [\"Latitude (°)\", \"Longitude (°)\", \"Horizontal Accuracy (m)\", \"Vertical Accuracy (m)\"]#, \"Height (m)\", \"Velocity (m/s), \"Direction (°)\",\"]\n",
    "linacc_cols = [ \"Linear Acceleration y (m/s^2)\", \"Linear Acceleration z (m/s^2)\", \"Linear Acceleration x (m/s^2)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n",
      "Skipped Barometer.csv: 'Time (s)' column not found or DataFrame is empty.\n"
     ]
    }
   ],
   "source": [
    "for cat in [\"Walking\", \"Sitting\", \"Cycling\", \"Sport\"]:\n",
    "    for f in os.listdir(\"data/\"):\n",
    "        if cat in f:\n",
    "            for subf in os.listdir(\"data/\" + f):\n",
    "\n",
    "                add_missing_datetime_column(\"data/\" + f + \"/\" + subf)\n",
    "                start_time = time.time()\n",
    "                subf_path = os.path.join(\"data\", f, subf)\n",
    "                if not os.path.isdir(subf_path):\n",
    "                    continue  # Skip if it's not a directory\n",
    "                #print(subf)\n",
    "                acc = pd.read_csv(\"data/\" + f + \"/\" + subf + \"/Accelerometer.csv\")\n",
    "                acc = correct_column_names(acc, column_corrections[\"Accelerometer\"])\n",
    "\n",
    "                gyro = pd.read_csv(\"data/\" + f + \"/\" + subf + \"/Gyroscope.csv\")\n",
    "                gyro = correct_column_names(gyro, column_corrections[\"Gyroscope\"])\n",
    "\n",
    "                linacc_path_1 = \"data/\" + f + \"/\" + subf + \"/Linear Acceleration.csv\"\n",
    "                linacc_path_2 = \"data/\" + f + \"/\" + subf + \"/Linear Accelerometer.csv\"\n",
    "                if os.path.exists(linacc_path_1):\n",
    "                    linacc = pd.read_csv(linacc_path_1)\n",
    "                elif os.path.exists(linacc_path_2):\n",
    "                    linacc = pd.read_csv(linacc_path_2)\n",
    "                else:\n",
    "                    #print(f\"Warning: Neither Linear Acceleration.csv nor Linear Accelerometer.csv found in {subf_path}.\")\n",
    "                    linacc = pd.DataFrame()  \n",
    "                linacc = correct_column_names(linacc, column_corrections[\"Linear Acceleration\"])\n",
    "\n",
    "                loc = pd.read_csv(\"data/\" + f + \"/\" + subf + \"/Location.csv\")\n",
    "                loc = correct_column_names(loc, column_corrections[\"Location\"])\n",
    "\n",
    "                cols = [\"Acceleration x (m/s^2)\", \"Acceleration y (m/s^2)\", \"Acceleration z (m/s^2)\",\n",
    "                        \"Gyroscope x (rad/s)\", \"Gyroscope y (rad/s)\", \"Gyroscope z (rad/s)\",\n",
    "                        \"Velocity (m/s)\", \"Height (m)\", \"Linear Acceleration x (m/s^2)\", \n",
    "                        \"Linear Acceleration y (m/s^2)\", \"Linear Acceleration z (m/s^2)\"]\n",
    "                \n",
    "                acc = remove_start_end(acc)\n",
    "                loc = remove_start_end(loc)\n",
    "                gyro = remove_start_end(gyro)\n",
    "                linacc = remove_start_end(linacc)\n",
    "\n",
    "                acc = acc.rename(columns={\"Datetime\": \"Datetime_acc\"})\n",
    "                loc = loc.rename(columns={\"Datetime\": \"Datetime_loc\"})\n",
    "                gyro = gyro.rename(columns={\"Datetime\": \"Datetime_gyro\"})\n",
    "                linacc = linacc.rename(columns={\"Datetime\": \"Datetime_linacc\"})\n",
    "        \n",
    "                df = pd.merge_asof(linacc, pd.merge_asof(loc, pd.merge_asof(acc, gyro, on=\"Time (s)\", direction=\"nearest\"), on=\"Time (s)\", direction=\"nearest\"), on=\"Time (s)\", direction=\"nearest\")\n",
    "                #Activity column\n",
    "                df['Activity'] = cat\n",
    "\n",
    "                #Remove rows with no Times (s)\n",
    "                df = drop_rows_with_no_time(df)\n",
    "\n",
    "                # Calculate LOF\n",
    "                df = calculate_lof(df, acc_cols, 'acc')\n",
    "                df = calculate_lof(df, gyro_cols, 'gyro')\n",
    "                df = calculate_lof(df, loc_cols, 'loc')\n",
    "                df = calculate_lof(df, linacc_cols, 'linacc')\n",
    "\n",
    "                # Remove outliers and impute\n",
    "                df = remove_outliers_and_impute(df, acc_cols, 'acc')\n",
    "                df = remove_outliers_and_impute(df, gyro_cols, 'gyro')\n",
    "                df = remove_outliers_and_impute(df, loc_cols, 'loc')\n",
    "                df = remove_outliers_and_impute(df, linacc_cols, 'linacc')\n",
    "\n",
    "                #df = df.drop(columns=['lof_acc', 'lof_gyro', 'lof_loc', 'lof_linacc'])\n",
    "\n",
    "                # Create the output directory if it doesn't exist\n",
    "                output_dir = \"data_processed/\" + f + \"/\" + subf\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "                # Write out the merged dataframe to CSV\n",
    "                df.to_csv(output_dir + f\"/{subf}.csv\", index=False)\n",
    "\n",
    "                end_time = time.time()\n",
    "                #print(f\"Processed {subf} in {f} for category {cat} in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_sort_nested_csv_files(base_directory, sort_column):\n",
    "    df_list = []\n",
    "    \n",
    "    # Traverse the base directory and read CSV files\n",
    "    for root, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                \n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames and sort by the specified column\n",
    "    concatenated_df = pd.concat(df_list, ignore_index=True)\n",
    "    sorted_df = concatenated_df.sort_values(by=sort_column)\n",
    "    return sorted_df\n",
    "\n",
    "# Concatenate and sort the CSV files by 'Datetime_linacc'\n",
    "sorted_nested_df = concatenate_and_sort_nested_csv_files(\"data_processed\", 'Datetime_linacc')\n",
    "\n",
    "# Save the sorted DataFrame to a new CSV file\n",
    "sorted_nested_output_file = 'data_processed/final_aggregated_output.csv'\n",
    "sorted_nested_df.to_csv(sorted_nested_output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove nulls\n",
    "df = pd.read_csv('data_processed/final_aggregated_output.csv')\n",
    "df_cleaned = df.dropna(subset=[\"Time (s)\"])\n",
    "df_cleaned.to_csv(\"data_processed/final_aggregated_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/g4tvh5tn2wb8t1qyc72n4cdr0000gn/T/ipykernel_52281/987721169.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Height (m)\"].fillna(0, inplace=True)\n",
      "/var/folders/_1/g4tvh5tn2wb8t1qyc72n4cdr0000gn/T/ipykernel_52281/987721169.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Velocity (m/s)\"].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_processed/final_aggregated_output.csv')\n",
    "df.drop(columns=[\"Direction (°)\"], inplace=True)\n",
    "\n",
    "# Replace every null value for \"height\" and \"velocity\" with 0\n",
    "df[\"Height (m)\"].fillna(0, inplace=True)\n",
    "df[\"Velocity (m/s)\"].fillna(0, inplace=True)\n",
    "df.to_csv(\"data_processed/final_aggregated_output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_processed/final_aggregated_output.csv')\n",
    "\n",
    "df = remove_outliers_and_impute(df, acc_cols, 'acc')\n",
    "df = remove_outliers_and_impute(df, gyro_cols, 'gyro')\n",
    "df = remove_outliers_and_impute(df, loc_cols, 'loc')\n",
    "df = remove_outliers_and_impute(df, linacc_cols, 'linacc')\n",
    "\n",
    "\n",
    "df.to_csv(f\"data_processed/final_aggregated_output_no_outlier.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_1/g4tvh5tn2wb8t1qyc72n4cdr0000gn/T/ipykernel_52281/1622116601.py:17: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  df_grouped_5s = df_filtered.groupby(pd.Grouper(freq='5S')).agg({**{col: 'mean' for col in numeric_columns}, 'Activity': 'first'})\n",
      "/var/folders/_1/g4tvh5tn2wb8t1qyc72n4cdr0000gn/T/ipykernel_52281/1622116601.py:22: FutureWarning: 'L' is deprecated and will be removed in a future version, please use 'ms' instead.\n",
      "  df_grouped_0_25s = df_filtered.groupby(pd.Grouper(freq='250L')).agg({**{col: 'mean' for col in numeric_columns}, 'Activity': 'first'})\n"
     ]
    }
   ],
   "source": [
    "#NBNB:: BROKEN WORK IN PRGRESS, IGNORE\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'data_processed/final_aggregated_output_no_outlier.csv'  # replace with your dataset path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the datetime column to a pandas datetime object\n",
    "df['Datetime_linacc'] = pd.to_datetime(df['Datetime_linacc'], errors='coerce')\n",
    "df = df.dropna(subset=['Datetime_linacc'])  # Drop rows where datetime conversion failed\n",
    "\n",
    "# Set the datetime column as the index\n",
    "df.set_index('Datetime_linacc', inplace=True)\n",
    "\n",
    "# Keep only numeric columns and the 'Activity' column\n",
    "numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "df_filtered = df[numeric_columns.to_list() + ['Activity']]\n",
    "\n",
    "# Group by 5-second intervals and aggregate\n",
    "df_grouped_5s = df_filtered.groupby(pd.Grouper(freq='5S')).agg({**{col: 'mean' for col in numeric_columns}, 'Activity': 'first'})\n",
    "# Drop rows where all values are NaN\n",
    "df_grouped_5s = df_grouped_5s.dropna(how='all')\n",
    "\n",
    "# Group by 0.25-second intervals and aggregate\n",
    "df_grouped_0_25s = df_filtered.groupby(pd.Grouper(freq='250L')).agg({**{col: 'mean' for col in numeric_columns}, 'Activity': 'first'})\n",
    "# Drop rows where all values are NaN\n",
    "df_grouped_0_25s = df_grouped_0_25s.dropna(how='all')\n",
    "\n",
    "# Save the aggregated data to new CSV files\n",
    "df_grouped_5s.to_csv('data_processed/grouped_5s.csv')\n",
    "df_grouped_0_25s.to_csv('data_processed/grouped_0_25s.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c7853a7f6380c5c652c4ecade97f8ac7cc0e9eab34a6ddd88c01f0f43192157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
